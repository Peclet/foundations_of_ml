{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa', 'versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "dataset = load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "target_names = list(dataset.target_names)\n",
    "print(target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = (y>1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: Linear Regression\n",
    "class LogReg:\n",
    "    \"\"\"\n",
    "    This implementation of Logistic Regression uses batch gradient descent with regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_iters=1000, tolerance = 1e-10, alpha=0.001, lambd=0, threshold=0.5, verbose=False):\n",
    "        self.num_iters = num_iters\n",
    "        self.alpha = alpha # Learning rate\n",
    "        self.lambd = lambd # Regularization parameter\n",
    "        self.tolerance = tolerance\n",
    "        self.threshold = threshold\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def add_ones(self, X):\n",
    "        return np.concatenate((np.ones((len(X),1)), X), axis = 1)\n",
    "      \n",
    "    def sigmoid(self, X, theta):\n",
    "        return 1/(1 + np.exp(X@theta))\n",
    "    \n",
    "    def cost(self, X, y_true):\n",
    "        m = X.shape[0]\n",
    "        y_hat = self.sigmoid(X, self.theta)\n",
    "        temp_theta = self.theta[:, 1:].copy()\n",
    "        \n",
    "        Cost = np.sum(-1*y_true*np.log(y_hat)-(1-y_true)*np.log(1-y_hat)) + self.lambd * np.sum(temp_theta**2)\n",
    "        \n",
    "        return Cost\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = X.copy()\n",
    "        X = self.add_ones(X)\n",
    "        \n",
    "        n, d = X.shape\n",
    "        \n",
    "        self.classes = np.unique(y)\n",
    "        self.no_classes = len(self.classes)\n",
    "        \n",
    "        # Turn y into one-hot-labels if number of classes is greater than 2\n",
    "        if self.no_classes > 2:\n",
    "            y_encode = np.zeros((n, self.no_classes))\n",
    "            y_encode[range(n), y] = 1 #numpy advanced indexing\n",
    "            y = y_encode\n",
    "        else:\n",
    "            y = y.reshape(-1, 1)        \n",
    "        \n",
    "        if self.no_classes > 2:\n",
    "            self.theta = np.zeros((d, self.no_classes))\n",
    "        else:\n",
    "            self.theta = np.zeros((d, 1))\n",
    "        \n",
    "        current_iter = 1\n",
    "        norm = 1\n",
    "        while (norm >= self.tolerance and current_iter < self.num_iters):\n",
    "            old_theta = self.theta.copy()\n",
    "            \n",
    "            temp_theta = self.theta[:, 1:].copy()\n",
    "            grad = X.T@(y - self.sigmoid(X, self.theta)) + self.lambd * np.sum(temp_theta)\n",
    "            \n",
    "            if self.no_classes <= 2:\n",
    "                grad= grad.reshape(-1, 1)\n",
    "            \n",
    "            self.theta = self.theta - self.alpha*grad\n",
    "            \n",
    "            if self.verbose and (current_iter%500 == 0):\n",
    "                print(f'cost for {current_iter} iteration : {self.cost(X, y)}')\n",
    "            norm = np.linalg.norm(old_theta - self.theta)\n",
    "            current_iter += 1\n",
    "            \n",
    "        return self.theta\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns mse loss for a dataset evaluated on the hypothesis\n",
    "        \"\"\"\n",
    "        X = self.add_ones(X)\n",
    "        return self.cost(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        print(proba.shape)\n",
    "        if self.no_classes > 2:\n",
    "            # Multiclass classification\n",
    "            y_hat = np.argmax(proba, axis=1)\n",
    "        elif self.no_classes == 2:\n",
    "            # Binary classification\n",
    "            y_hat = (proba >= self.threshold).astype(int)\n",
    "        return y_hat\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Returns probability of predictions.\n",
    "        \"\"\"\n",
    "        X = self.add_ones(X)\n",
    "        \n",
    "        return self.sigmoid(X, self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogReg(num_iters=30000, alpha=0.001, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for 500 iteration : 100.8893466253054\n",
      "cost for 1000 iteration : 93.55043991296125\n",
      "cost for 1500 iteration : 90.4218146105867\n",
      "cost for 2000 iteration : 88.55315404898136\n",
      "cost for 2500 iteration : 87.26301539831763\n",
      "cost for 3000 iteration : 86.30342396795784\n",
      "cost for 3500 iteration : 85.55702321969936\n",
      "cost for 4000 iteration : 84.95868830753454\n",
      "cost for 4500 iteration : 84.46840852565802\n",
      "cost for 5000 iteration : 84.05981985747101\n",
      "cost for 5500 iteration : 83.71463780861362\n",
      "cost for 6000 iteration : 83.41967801922067\n",
      "cost for 6500 iteration : 83.16514118749294\n",
      "cost for 7000 iteration : 82.94356714012383\n",
      "cost for 7500 iteration : 82.74916598970448\n",
      "cost for 8000 iteration : 82.57737306956281\n",
      "cost for 8500 iteration : 82.42454263861949\n",
      "cost for 9000 iteration : 82.28773098078418\n",
      "cost for 9500 iteration : 82.16453903552011\n",
      "cost for 10000 iteration : 82.05299583890223\n",
      "cost for 10500 iteration : 81.9514706587301\n",
      "cost for 11000 iteration : 81.85860575462758\n",
      "cost for 11500 iteration : 81.77326425077104\n",
      "cost for 12000 iteration : 81.69448926904806\n",
      "cost for 12500 iteration : 81.62147157577371\n",
      "cost for 13000 iteration : 81.55352374790935\n",
      "cost for 13500 iteration : 81.49005938809776\n",
      "cost for 14000 iteration : 81.43057628848054\n",
      "cost for 14500 iteration : 81.37464271017458\n",
      "cost for 15000 iteration : 81.32188614038691\n",
      "cost for 15500 iteration : 81.27198403369309\n",
      "cost for 16000 iteration : 81.22465615239523\n",
      "cost for 16500 iteration : 81.17965820304647\n",
      "cost for 17000 iteration : 81.13677652913323\n",
      "cost for 17500 iteration : 81.09582366849735\n",
      "cost for 18000 iteration : 81.05663462191266\n",
      "cost for 18500 iteration : 81.01906370890943\n",
      "cost for 19000 iteration : 80.98298191037526\n",
      "cost for 19500 iteration : 80.94827461608554\n",
      "cost for 20000 iteration : 80.91483971019709\n",
      "cost for 20500 iteration : 80.88258593969542\n",
      "cost for 21000 iteration : 80.85143152043744\n",
      "cost for 21500 iteration : 80.82130294325825\n",
      "cost for 22000 iteration : 80.79213394898495\n",
      "cost for 22500 iteration : 80.76386464641334\n",
      "cost for 23000 iteration : 80.73644075157813\n",
      "cost for 23500 iteration : 80.70981293017213\n",
      "cost for 24000 iteration : 80.68393622787767\n",
      "cost for 24500 iteration : 80.65876957578666\n",
      "cost for 25000 iteration : 80.63427536008986\n",
      "cost for 25500 iteration : 80.61041904688591\n",
      "cost for 26000 iteration : 80.58716885435791\n",
      "cost for 26500 iteration : 80.56449546573357\n",
      "cost for 27000 iteration : 80.54237177742796\n",
      "cost for 27500 iteration : 80.52077267759327\n",
      "cost for 28000 iteration : 80.49967485099741\n",
      "cost for 28500 iteration : 80.47905660674203\n",
      "cost for 29000 iteration : 80.45889772582922\n",
      "cost for 29500 iteration : 80.43917932600837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.62738981, -7.31876769,  9.99124547],\n",
       "       [-0.99903598,  0.23820684,  4.1390738 ],\n",
       "       [-3.48622877,  2.78764351,  5.06871866],\n",
       "       [ 5.45243564, -1.31128207, -6.85186537],\n",
       "       [ 2.55914269,  2.77782575, -9.96363615]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 3)\n"
     ]
    }
   ],
   "source": [
    "predictions = logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y == predictions.squeeze()) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
