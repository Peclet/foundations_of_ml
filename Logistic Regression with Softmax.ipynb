{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa', 'versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "dataset = load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "target_names = list(dataset.target_names)\n",
    "print(target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg:\n",
    "    \"\"\"\n",
    "    This implementation of Logistic Regression uses batch gradient descent with regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_iters=1000, tolerance = 1e-10, alpha=0.001, lambd=0, threshold=0.5, verbose=False):\n",
    "        self.num_iters = num_iters\n",
    "        self.alpha = alpha # Learning rate\n",
    "        self.lambd = lambd # Regularization parameter\n",
    "        self.tolerance = tolerance\n",
    "        self.threshold = threshold\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def add_ones(self, X):\n",
    "        return np.concatenate((np.ones((len(X),1)), X), axis = 1)\n",
    "      \n",
    "    def sigmoid(self, X, theta):\n",
    "        return 1/(1 + np.exp(X@theta))\n",
    "    \n",
    "    def cost(self, X, y_true):\n",
    "        m = X.shape[0]\n",
    "        y_hat = self.sigmoid(X, self.theta)\n",
    "        temp_theta = self.theta[:, 1:].copy()\n",
    "        \n",
    "        Cost = np.sum(-1*y_true*np.log(y_hat)-(1-y_true)*np.log(1-y_hat)) + self.lambd * np.sum(temp_theta**2)\n",
    "        \n",
    "        return Cost\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = X.copy()\n",
    "        X = self.add_ones(X)\n",
    "        \n",
    "        n, d = X.shape\n",
    "        \n",
    "        self.classes = np.unique(y)\n",
    "        self.no_classes = len(self.classes)\n",
    "        \n",
    "        # Turn y into one-hot-labels if number of classes is greater than 2\n",
    "        if self.no_classes > 2:\n",
    "            y_encode = np.zeros((n, self.no_classes))\n",
    "            y_encode[range(n), y] = 1 #numpy advanced indexing\n",
    "            y = y_encode\n",
    "        else:\n",
    "            y = y.reshape(-1, 1)        \n",
    "        \n",
    "        if self.no_classes > 2:\n",
    "            self.theta = np.zeros((d, self.no_classes))\n",
    "        else:\n",
    "            self.theta = np.zeros((d, 1))\n",
    "        \n",
    "        current_iter = 1\n",
    "        norm = 1\n",
    "        while (norm >= self.tolerance and current_iter < self.num_iters):\n",
    "            old_theta = self.theta.copy()\n",
    "            \n",
    "            temp_theta = self.theta[:, 1:].copy()\n",
    "            grad = X.T@(y - self.sigmoid(X, self.theta)) + self.lambd * np.sum(temp_theta)\n",
    "            \n",
    "            if self.no_classes <= 2:\n",
    "                grad= grad.reshape(-1, 1)\n",
    "            \n",
    "            self.theta = self.theta - self.alpha*grad\n",
    "            \n",
    "            if self.verbose and (current_iter%500 == 0):\n",
    "                print(f'cost for {current_iter} iteration : {self.cost(X, y)}')\n",
    "            norm = np.linalg.norm(old_theta - self.theta)\n",
    "            current_iter += 1\n",
    "            \n",
    "        return self.theta\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns mse loss for a dataset evaluated on the hypothesis\n",
    "        \"\"\"\n",
    "        X = self.add_ones(X)\n",
    "        return self.cost(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        if self.no_classes > 2:\n",
    "            # Multiclass classification\n",
    "            y_hat = np.argmax(proba, axis=1)\n",
    "        elif self.no_classes == 2:\n",
    "            # Binary classification\n",
    "            y_hat = (proba >= self.threshold).astype(int)\n",
    "        return y_hat\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Returns probability of predictions.\n",
    "        \"\"\"\n",
    "        X = self.add_ones(X)\n",
    "        \n",
    "        return self.sigmoid(X, self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Mini-Batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg:\n",
    "    \"\"\"\n",
    "    This implementation of Logistic Regression uses batch gradient descent with regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 epochs=100, \n",
    "                 tolerance = 1e-10, \n",
    "                 alpha=0.001, \n",
    "                 lambd=0, \n",
    "                 threshold=0.5, \n",
    "                 verbose=False,\n",
    "                 minibatch_size=30,\n",
    "                ):\n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha # Learning rate\n",
    "        self.lambd = lambd # Regularization parameter\n",
    "        self.tolerance = tolerance\n",
    "        self.threshold = threshold\n",
    "        self.verbose = verbose\n",
    "        self.minibatch_size = minibatch_size\n",
    "        \n",
    "    def add_ones(self, X):\n",
    "        return np.concatenate((np.ones((len(X),1)), X), axis = 1)\n",
    "      \n",
    "    def sigmoid(self, X, theta):\n",
    "        return 1/(1 + np.exp(X@theta))\n",
    "    \n",
    "    def cost(self, X, y_true):\n",
    "        m = X.shape[0]\n",
    "        y_hat = self.sigmoid(X, self.theta)\n",
    "        temp_theta = self.theta[:, 1:].copy()\n",
    "        \n",
    "        Cost = np.sum(-1*y_true*np.log(y_hat)-(1-y_true)*np.log(1-y_hat)) + self.lambd * np.sum(temp_theta**2)\n",
    "        \n",
    "        return Cost\n",
    "    \n",
    "    def get_minibatch(self, X, y,  minibatch):\n",
    "        X_mb = X[minibatch*self.minibatch_size: (minibatch+1)*self.minibatch_size]\n",
    "        y_mb = y[minibatch*self.minibatch_size: (minibatch+1)*self.minibatch_size]\n",
    "        return X_mb, y_mb\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = X.copy()\n",
    "        X = self.add_ones(X)\n",
    "        \n",
    "        n, d = X.shape\n",
    "        \n",
    "        self.classes = np.unique(y)\n",
    "        self.no_classes = len(self.classes)\n",
    "        \n",
    "        # Turn y into one-hot-labels if number of classes is greater than 2\n",
    "        if self.no_classes > 2:\n",
    "            y_encode = np.zeros((n, self.no_classes))\n",
    "            y_encode[range(n), y] = 1 #numpy advanced indexing\n",
    "            y = y_encode\n",
    "        else:\n",
    "            y = y.reshape(-1, 1)        \n",
    "        \n",
    "        if self.no_classes > 2:\n",
    "            self.theta = np.zeros((d, self.no_classes))\n",
    "        else:\n",
    "            self.theta = np.zeros((d, 1))\n",
    "        \n",
    "        current_epoch = 1\n",
    "        norm = 1\n",
    "        \n",
    "        no_of_minibatch = int(n/self.minibatch_size)\n",
    "        \n",
    "        while (norm >= self.tolerance and current_epoch < self.epochs):\n",
    "            # Shuffle X for minibatch gradient descent\n",
    "            shuffled_index = np.random.permutation(n)\n",
    "            X_shuffled = X[shuffled_index]\n",
    "            y_shuffled = y[shuffled_index]\n",
    "            \n",
    "            old_theta = self.theta.copy()\n",
    "            theta_wo_bias = self.theta[:, 1:].copy()\n",
    "            \n",
    "            for mb in range(no_of_minibatch):\n",
    "                X_mb, y_mb = self.get_minibatch(X, y, mb)\n",
    "                \n",
    "                grad = X_mb.T@(y_mb - self.sigmoid(X_mb, self.theta)) + self.lambd * np.sum(theta_wo_bias)\n",
    "                \n",
    "                if self.no_classes <= 2:\n",
    "                    grad= grad.reshape(-1, 1)\n",
    "            \n",
    "                self.theta = self.theta - self.alpha*grad\n",
    "            \n",
    "            if self.verbose and (current_epoch%100 == 0):\n",
    "                print(f'cost for {current_epoch} epoch : {self.cost(X, y)}')\n",
    "            norm = np.linalg.norm(old_theta - self.theta)\n",
    "            current_epoch += 1\n",
    "            \n",
    "        return self.theta\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns mse loss for a dataset evaluated on the hypothesis\n",
    "        \"\"\"\n",
    "        X = self.add_ones(X)\n",
    "        return self.cost(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        if self.no_classes > 2:\n",
    "            # Multiclass classification\n",
    "            y_hat = np.argmax(proba, axis=1)\n",
    "        elif self.no_classes == 2:\n",
    "            # Binary classification\n",
    "            y_hat = (proba >= self.threshold).astype(int)\n",
    "        return y_hat\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Returns probability of predictions.\n",
    "        \"\"\"\n",
    "        X = self.add_ones(X)\n",
    "        \n",
    "        return self.sigmoid(X, self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogReg(epochs=9000, \n",
    "                alpha=0.001, \n",
    "                verbose=True, \n",
    "                minibatch_size=20,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for 100 epoch : 135.59525985436156\n",
      "cost for 200 epoch : 121.23009983356812\n",
      "cost for 300 epoch : 113.79842079017831\n",
      "cost for 400 epoch : 109.12447470126725\n",
      "cost for 500 epoch : 105.89970812105123\n",
      "cost for 600 epoch : 103.53725844231357\n",
      "cost for 700 epoch : 101.72803765520975\n",
      "cost for 800 epoch : 100.29308217606992\n",
      "cost for 900 epoch : 99.12191340628333\n",
      "cost for 1000 epoch : 98.14301442596208\n",
      "cost for 1100 epoch : 97.30832151431306\n",
      "cost for 1200 epoch : 96.58451643840516\n",
      "cost for 1300 epoch : 95.9478825413426\n",
      "cost for 1400 epoch : 95.38114079198942\n",
      "cost for 1500 epoch : 94.87143794739103\n",
      "cost for 1600 epoch : 94.40903094350807\n",
      "cost for 1700 epoch : 93.98640555707738\n",
      "cost for 1800 epoch : 93.5976733701057\n",
      "cost for 1900 epoch : 93.23815132045026\n",
      "cost for 2000 epoch : 92.90406354016872\n",
      "cost for 2100 epoch : 92.59232661496557\n",
      "cost for 2200 epoch : 92.3003926991976\n",
      "cost for 2300 epoch : 92.02613336297057\n",
      "cost for 2400 epoch : 91.7677525139346\n",
      "cost for 2500 epoch : 91.52372033949058\n",
      "cost for 2600 epoch : 91.29272262888432\n",
      "cost for 2700 epoch : 91.07362147556215\n",
      "cost for 2800 epoch : 90.86542449070079\n",
      "cost for 2900 epoch : 90.66726044737516\n",
      "cost for 3000 epoch : 90.47835983114018\n",
      "cost for 3100 epoch : 90.29803916944877\n",
      "cost for 3200 epoch : 90.12568829795848\n",
      "cost for 3300 epoch : 89.96075992940395\n",
      "cost for 3400 epoch : 89.8027610429865\n",
      "cost for 3500 epoch : 89.6512457248708\n",
      "cost for 3600 epoch : 89.50580917438666\n",
      "cost for 3700 epoch : 89.36608265368906\n",
      "cost for 3800 epoch : 89.23172920647876\n",
      "cost for 3900 epoch : 89.10244000791104\n",
      "cost for 4000 epoch : 88.97793123590955\n",
      "cost for 4100 epoch : 88.85794137585847\n",
      "cost for 4200 epoch : 88.74222888761739\n",
      "cost for 4300 epoch : 88.63057017713317\n",
      "cost for 4400 epoch : 88.52275782546026\n",
      "cost for 4500 epoch : 88.41859903639082\n",
      "cost for 4600 epoch : 88.31791427061023\n",
      "cost for 4700 epoch : 88.22053603970731\n",
      "cost for 4800 epoch : 88.12630783775452\n",
      "cost for 4900 epoch : 88.03508319174782\n",
      "cost for 5000 epoch : 87.9467248151262\n",
      "cost for 5100 epoch : 87.86110385100409\n",
      "cost for 5200 epoch : 87.77809919374586\n",
      "cost for 5300 epoch : 87.69759687917593\n",
      "cost for 5400 epoch : 87.6194895351018\n",
      "cost for 5500 epoch : 87.54367588499494\n",
      "cost for 5600 epoch : 87.47006029865415\n",
      "cost for 5700 epoch : 87.39855238450369\n",
      "cost for 5800 epoch : 87.32906661888525\n",
      "cost for 5900 epoch : 87.26152200829947\n",
      "cost for 6000 epoch : 87.19584178106632\n",
      "cost for 6100 epoch : 87.1319531053123\n",
      "cost for 6200 epoch : 87.06978683056973\n",
      "cost for 6300 epoch : 87.00927725059871\n",
      "cost for 6400 epoch : 86.95036188532433\n",
      "cost for 6500 epoch : 86.89298128002406\n",
      "cost for 6600 epoch : 86.83707882011481\n",
      "cost for 6700 epoch : 86.78260056007102\n",
      "cost for 6800 epoch : 86.72949506516848\n",
      "cost for 6900 epoch : 86.67771326488887\n",
      "cost for 7000 epoch : 86.62720831694432\n",
      "cost for 7100 epoch : 86.57793548098975\n",
      "cost for 7200 epoch : 86.52985200118688\n",
      "cost for 7300 epoch : 86.48291699686897\n",
      "cost for 7400 epoch : 86.43709136062957\n",
      "cost for 7500 epoch : 86.39233766322538\n",
      "cost for 7600 epoch : 86.34862006474214\n",
      "cost for 7700 epoch : 86.30590423152509\n",
      "cost for 7800 epoch : 86.26415725842213\n",
      "cost for 7900 epoch : 86.2233475959294\n",
      "cost for 8000 epoch : 86.18344498186677\n",
      "cost for 8100 epoch : 86.14442037724385\n",
      "cost for 8200 epoch : 86.10624590600668\n",
      "cost for 8300 epoch : 86.06889479838316\n",
      "cost for 8400 epoch : 86.03234133756871\n",
      "cost for 8500 epoch : 85.9965608095161\n",
      "cost for 8600 epoch : 85.9615294556121\n",
      "cost for 8700 epoch : 85.9272244280431\n",
      "cost for 8800 epoch : 85.89362374766628\n",
      "cost for 8900 epoch : 85.86070626421828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.52603372, -5.82116714,  5.08083386],\n",
       "       [-0.82769709, -0.28662784,  4.14958303],\n",
       "       [-2.94848302,  2.82319963,  4.1482948 ],\n",
       "       [ 4.5690382 , -0.72198485, -6.41348991],\n",
       "       [ 2.08991479,  2.40110809, -6.87615605]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 3)\n"
     ]
    }
   ],
   "source": [
    "predictions = logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y == predictions.squeeze()) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, 3, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "x = np.array([2, 4, 5, 9, 10])\n",
    "\n",
    "it = np.random.permutation(5)\n",
    "it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  2,  4,  9, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
