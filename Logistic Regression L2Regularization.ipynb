{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa', 'versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "dataset = load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "target_names = list(dataset.target_names)\n",
    "print(target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change to binary class\n",
    "y = (y > 0).astype(int)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg:\n",
    "    \"\"\"\n",
    "    This implementation of Logistic Regression uses mini-batch gradient descent with l2-regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 epochs=100, \n",
    "                 tolerance = 1e-10, \n",
    "                 alpha=0.001, \n",
    "                 lambd=0, \n",
    "                 threshold=0.5, \n",
    "                 verbose=False,\n",
    "                 minibatch_size=30,\n",
    "                ):\n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha # Learning rate\n",
    "        self.lambd = lambd # Regularization parameter\n",
    "        self.tolerance = tolerance\n",
    "        self.threshold = threshold\n",
    "        self.verbose = verbose\n",
    "        self.minibatch_size = minibatch_size\n",
    "        \n",
    "    def add_ones(self, X):\n",
    "        return np.concatenate((np.ones((len(X),1)), X), axis = 1)\n",
    "      \n",
    "    def sigmoid(self, X, theta):\n",
    "        return 1/(1 + np.exp(X@theta))\n",
    "    \n",
    "    def cost(self, X, y_true):\n",
    "        m = X.shape[0]\n",
    "        y_hat = self.sigmoid(X, self.theta)\n",
    "        temp_theta = self.theta[:, 1:].copy()\n",
    "        \n",
    "        Cost = np.sum(-1*y_true*np.log(y_hat)-(1-y_true)*np.log(1-y_hat)) + self.lambd * np.sum(temp_theta**2)\n",
    "        \n",
    "        return Cost\n",
    "    \n",
    "    def get_minibatch(self, X, y,  minibatch):\n",
    "        X_mb = X[minibatch*self.minibatch_size: (minibatch+1)*self.minibatch_size]\n",
    "        y_mb = y[minibatch*self.minibatch_size: (minibatch+1)*self.minibatch_size]\n",
    "        return X_mb, y_mb\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = X.copy()\n",
    "        X = self.add_ones(X)\n",
    "        \n",
    "        n, d = X.shape\n",
    "        \n",
    "        self.classes = np.unique(y)\n",
    "        self.no_classes = len(self.classes)\n",
    "        \n",
    "        # Turn y into one-hot-labels if number of classes is greater than 2\n",
    "        if self.no_classes > 2:\n",
    "            y_encode = np.zeros((n, self.no_classes))\n",
    "            y_encode[range(n), y] = 1 #numpy advanced indexing\n",
    "            y = y_encode\n",
    "        else:\n",
    "            y = y.reshape(-1, 1)        \n",
    "        \n",
    "        if self.no_classes > 2:\n",
    "            self.theta = np.zeros((d, self.no_classes))\n",
    "        else:\n",
    "            self.theta = np.zeros((d, 1))\n",
    "        \n",
    "        current_epoch = 1\n",
    "        norm = 1\n",
    "        \n",
    "        no_of_minibatch = int(n/self.minibatch_size)\n",
    "        \n",
    "        while (norm >= self.tolerance and current_epoch < self.epochs):\n",
    "            # Shuffle X for minibatch gradient descent\n",
    "            shuffled_index = np.random.permutation(n)\n",
    "            X_shuffled = X[shuffled_index]\n",
    "            y_shuffled = y[shuffled_index]\n",
    "            \n",
    "            old_theta = self.theta.copy()\n",
    "            theta_wo_bias = self.theta[:, 1:].copy()\n",
    "            \n",
    "            for mb in range(no_of_minibatch):\n",
    "                X_mb, y_mb = self.get_minibatch(X, y, mb)\n",
    "                \n",
    "                grad = X_mb.T@(y_mb - self.sigmoid(X_mb, self.theta)) + self.lambd * np.sum(theta_wo_bias)\n",
    "                \n",
    "                if self.no_classes <= 2:\n",
    "                    grad= grad.reshape(-1, 1)\n",
    "            \n",
    "                self.theta = self.theta - self.alpha*grad\n",
    "            \n",
    "            if self.verbose and (current_epoch%100 == 0):\n",
    "                print(f'cost for {current_epoch} epoch : {self.cost(X, y)}')\n",
    "            norm = np.linalg.norm(old_theta - self.theta)\n",
    "            current_epoch += 1\n",
    "            \n",
    "        return self.theta\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns mse loss for a dataset evaluated on the hypothesis\n",
    "        \"\"\"\n",
    "        X = self.add_ones(X)\n",
    "        return self.cost(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        if self.no_classes > 2:\n",
    "            # Multiclass classification\n",
    "            y_hat = np.argmax(proba, axis=1)\n",
    "        elif self.no_classes == 2:\n",
    "            # Binary classification\n",
    "            y_hat = (proba >= self.threshold).astype(int)\n",
    "        return y_hat\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Returns probability of predictions.\n",
    "        \"\"\"\n",
    "        X = self.add_ones(X)\n",
    "        \n",
    "        return self.sigmoid(X, self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogReg(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.200352  ],\n",
       "       [ 0.30345859],\n",
       "       [ 1.08722447],\n",
       "       [-1.71817827],\n",
       "       [-0.76690365]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logreg.predict(X)\n",
    "predictions = predictions.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y == predictions) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
