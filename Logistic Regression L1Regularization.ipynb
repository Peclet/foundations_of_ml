{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa', 'versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "dataset = load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "target_names = list(dataset.target_names)\n",
    "print(target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change to binary class\n",
    "y = (y > 0).astype(int)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: Linear Regression\n",
    "class LogReg:\n",
    "    \"\"\"\n",
    "    This implementation of Logistic Regression uses batch gradient descent with regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_iters=100, tolerance = 1e-10, alpha=0.00001, lambd=10, threshold=0.5, verbose=False):\n",
    "        self.num_iters = num_iters\n",
    "        self.alpha = alpha # Learning rate\n",
    "        self.lambd = lambd # Regularization parameter\n",
    "        self.tolerance = tolerance\n",
    "        self.threshold = threshold\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def add_ones(self, X):\n",
    "        return np.concatenate((np.ones((len(X),1)), X), axis = 1)\n",
    "      \n",
    "    def sigmoid(self, X, theta):\n",
    "        return 1/(1 + np.exp(X@theta))\n",
    "    \n",
    "    def cost(self, X, y_true):\n",
    "        m = X.shape[0]\n",
    "        y_hat = self.sigmoid(X, self.theta)\n",
    "        temp_theta = self.theta[1:].copy()\n",
    "        \n",
    "        Cost = np.sum(-1*y_true*np.log(y_hat)-(1-y_true)*np.log(1-y_hat)) + self.lambd * np.sum(np.abs(temp_theta))\n",
    "        \n",
    "        return Cost\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = X.copy()\n",
    "        X = self.add_ones(X)\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "        self.theta = np.zeros((len(X[0]), 1))\n",
    "        \n",
    "        current_iter = 1\n",
    "        norm = 1\n",
    "        while (norm >= self.tolerance and current_iter < self.num_iters):\n",
    "            old_theta = self.theta.copy()\n",
    "            #grad = np.dot(np.transpose(y_hat-self.y), self.X)\n",
    "            temp_theta = self.theta[1:].copy()\n",
    "            \n",
    "            grad = X.T@(y - self.sigmoid(X, self.theta)) + self.lambd * np.sum(np.sign(temp_theta))\n",
    "            grad= grad.reshape(-1, 1)\n",
    "            \n",
    "            self.theta = self.theta - self.alpha*grad\n",
    "            \n",
    "            if self.verbose and (current_iter%100 == 0):\n",
    "                print(f'cost for {current_iter} iteration : {self.cost(X, y)}')\n",
    "            norm = np.linalg.norm(old_theta - self.theta)\n",
    "            current_iter += 1\n",
    "            \n",
    "        return self.theta\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns mse loss for a dataset evaluated on the hypothesis\n",
    "        \"\"\"\n",
    "        X = self.add_ones(X)\n",
    "        return self.cost(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prob = self.predict_proba(X)\n",
    "        return (prob > self.threshold).astype(int)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Returns probability of predictions.\n",
    "        \"\"\"\n",
    "        X = self.add_ones(X)  \n",
    "        return self.sigmoid(X, self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogReg(verbose=True, num_iters=5500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost for 100 iteration : 81.86845645452154\n",
      "cost for 200 iteration : 75.69639362386795\n",
      "cost for 300 iteration : 70.86446051377749\n",
      "cost for 400 iteration : 66.75641561522221\n",
      "cost for 500 iteration : 63.229183428839\n",
      "cost for 600 iteration : 60.19046541639731\n",
      "cost for 700 iteration : 57.565248049021044\n",
      "cost for 800 iteration : 55.29047068140731\n",
      "cost for 900 iteration : 53.313106222444105\n",
      "cost for 1000 iteration : 51.58870719451656\n",
      "cost for 1100 iteration : 50.08008738480574\n",
      "cost for 1200 iteration : 48.75614167476681\n",
      "cost for 1300 iteration : 47.59082189285532\n",
      "cost for 1400 iteration : 46.56226869803543\n",
      "cost for 1500 iteration : 45.65208702420064\n",
      "cost for 1600 iteration : 44.844747154127276\n",
      "cost for 1700 iteration : 44.1270924149807\n",
      "cost for 1800 iteration : 43.487935655357575\n",
      "cost for 1900 iteration : 42.917728778764314\n",
      "cost for 2000 iteration : 42.4082919663456\n",
      "cost for 2100 iteration : 41.9525914749114\n",
      "cost for 2200 iteration : 41.54455689633133\n",
      "cost for 2300 iteration : 41.17893046775424\n",
      "cost for 2400 iteration : 40.85114243711886\n",
      "cost for 2500 iteration : 40.557207645706534\n",
      "cost for 2600 iteration : 40.29363942680733\n",
      "cost for 2700 iteration : 40.057377674244975\n",
      "cost for 2800 iteration : 39.84572854011334\n",
      "cost for 2900 iteration : 39.65631370633404\n",
      "cost for 3000 iteration : 39.487027563398534\n",
      "cost for 3100 iteration : 39.33600094133725\n",
      "cost for 3200 iteration : 39.201570288206256\n",
      "cost for 3300 iteration : 39.082251392702446\n",
      "cost for 3400 iteration : 38.976716909852804\n",
      "cost for 3500 iteration : 38.88377707996224\n",
      "cost for 3600 iteration : 38.80236313739982\n",
      "cost for 3700 iteration : 38.731512992308666\n",
      "cost for 3800 iteration : 38.46312913819298\n",
      "cost for 3900 iteration : 38.219485982962226\n",
      "cost for 4000 iteration : 38.06760277671894\n",
      "cost for 4100 iteration : 37.956114504221716\n",
      "cost for 4200 iteration : 37.86871961521217\n",
      "cost for 4300 iteration : 37.79388949634007\n",
      "cost for 4400 iteration : 37.733242246065615\n",
      "cost for 4500 iteration : 37.68730823827111\n",
      "cost for 4600 iteration : 37.64295093128989\n",
      "cost for 4700 iteration : 37.60490726356989\n",
      "cost for 4800 iteration : 37.57828803475554\n",
      "cost for 4900 iteration : 37.54977459599251\n",
      "cost for 5000 iteration : 37.53038009701258\n",
      "cost for 5100 iteration : 37.51486544839557\n",
      "cost for 5200 iteration : 37.50590640840867\n",
      "cost for 5300 iteration : 37.50007666783164\n",
      "cost for 5400 iteration : 37.49561637361824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.8499],\n",
       "       [-0.11  ],\n",
       "       [ 1.1056],\n",
       "       [-1.322 ],\n",
       "       [-0.    ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logreg.predict(X)\n",
    "predictions = predictions.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y == predictions) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
